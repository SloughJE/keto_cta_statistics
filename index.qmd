---
# title: "KETO-CTA: 'Plaque begets plaque, ApoB does not'"
# subtitle: "The Statistics Cause Doubt"
author: "John Slough"
format:
  revealjs:
    self-contained: true
    theme: [night]
    slide-number: true
    
---

<div style="font-size: 200%; margin-top: 1.5em; text-align: center; font-weight: bold;">
Plaque Begets Plaque, ApoB Does Not
</div>

<div style="margin-top: 1.2em; margin-bottom: 1.5em; text-align: center; font-weight: bold; font-size: 120%;">
The Statistics Cause Doubt
</div>

<hr style="width: 45%; margin: 2em auto 2em auto; border: 0.5px solid #888;">

<div style="text-align: center; font-size: 90%;">
John Slough
</div>


---

## Study Summary {.smaller}
[Plaque Begets Plaque, ApoB Does Not — Soto-Mota et al., 2025](https://www.jacc.org/doi/10.1016/j.jacadv.2025.101686)

- **Trial ID**: [NCT05733325](https://clinicaltrials.gov/study/NCT05733325)
<br>

- **Design**: 1-year prospective cohort using coronary CT angiography (CCTA)  
- **Participants**: 100 lean, metabolically healthy adults on keto ≥2 yrs  
  - LDL-C ≥190 mg/dL, HDL-C ≥60 mg/dL, TG ≤80 mg/dL, ApoB 185 ± 51 mg/dL  
  <br>

::: {.fragment}
  
- **Findings**: No link between ApoB/LDL-C (baseline/change) and plaque progression  
- **Predictors**: Baseline plaque (CAC, NCPV, TPS, PAV) strongly predicted progression (ΔNCPV) 
- **Stats**: Bayesian analysis favored no ApoB–plaque link (6–10× vs alt)  
<br>
- **Conclusion**: *Plaque begets plaque; ApoB does not*  

:::

---

<div style="font-size: 170%; line-height: 1.4;">

1. ΔNCPV as Outcome Variable  
2. Univariable Linear Models  
3. Bayesian Inference  
4. Bayesian Prior Choice  
5. Bayes Factor Interpretation 

</div>


# ΔNCPV as Outcome Variable
**Plaque begets plaque**: biology or mathematical artifact?


## 1 ΔNCPV as Outcome Variable {.smaller}


> "All baseline plaque metrics (coronary artery calcium, NCPV, total plaque score, and percent atheroma volume) were strongly associated with the **change in NCPV**."

::: {.fragment}


Change in noncalcified plaque volume $\Delta \text{NCPV}$ was the outcome:

$$
\Delta \text{NCPV} = \text{NCPV}_{1} - \text{NCPV}_0
$$

They regressed $\Delta \text{NCPV}$ directly on its baseline value $\text{NCPV}_0$:  

$$
\Delta \text{NCPV} = \alpha + \beta \, \text{NCPV}_0 + \varepsilon
$$

But this introduces **mathematical coupling**, because $\text{NCPV}_0$ appears on both sides of the equation:

$$
\text{NCPV}_{1} - \text{NCPV}_0 = \alpha + \beta \, \text{NCPV}_0 + \varepsilon
$$

:::

---

## 1 ΔNCPV as Outcome Variable {.smaller}

<div style="font-size: 75%; line-height: 1.3;">

["Mathematical coupling occurs when one variable directly or indirectly contains the whole or part of another, and the two variables are then analysed using correlation or regression. As a result, the statistical procedure of testing the null hypothesis — that the coefficient of correlation or the slope of regression is zero — might no longer be appropriate."](https://pubmed.ncbi.nlm.nih.gov/16526009/) - Tu & Gilthorpe, 2007

</div>


<br>

::: {.fragment}

<div style="font-size: 80%; line-height: 1.3;">

Regression model: $\text{NCPV}_{1} - \text{NCPV}_0 = \alpha + \beta \, \text{NCPV}_0 + \varepsilon$

<br>
The regression coefficient (slope): $\beta = \frac{\operatorname{Cov}((\text{NCPV}_1 - \text{NCPV}_0),\ \text{NCPV}_0)}{\operatorname{Var}(\text{NCPV}_0)} = \frac{\operatorname{Cov}(\Delta \text{NCPV},\ \text{NCPV}_0)}{\operatorname{Var}(\text{NCPV}_0)}$  

<br>
This simplifies to: $\beta = \frac{\rho\, \sigma_1 - \sigma_0}{\sigma_0}$  

where:

- $\rho = \operatorname{Cor}(\text{NCPV}_0,\ \text{NCPV}_1)$  
- $\sigma_0$, $\sigma_1$ = SDs at baseline and follow-up

</div>

:::

::: {.fragment}

<div style="font-size: 80%; line-height: 1.3;">

With baseline NCPV contributing to both predictor and outcome, the slope captures an **inseparable combination** of mathematical coupling and possible biological change. <br>
It is not a clean estimate of baseline influence.

</div>

:::


---

## 1 ΔNCPV as Outcome Variable {.smaller}

<div style="font-size: 90%; ">

From Oldham* (1962): $\beta > 0 \quad\text{if}\quad \rho > \frac{\sigma_0}{\sigma_1}$

The slope depends on:

- **correlation** between baseline and follow-up ($\rho$)
- **relative spread** of the two time points

::: {.fragment}

So a positive or negative slope arises **purely from the math**.

In this study:

- Almost all participants had increased NCPV
- Implies $\rho$ was moderate-to-high, and $\sigma_1 > \sigma_0$

These conditions tend to bias the slope upward. 

:::

::: {.fragment}

The slope reflects a mix of true correlation ($\rho$), variability ratio ($\sigma_1/\sigma_0$), and structural bias from subtracting $NCPV_0$ from both sides.

:::

</div>

::: {.fragment}

<br>
<small>\*Source: [Oldham, 1962](https://www.sciencedirect.com/science/article/abs/pii/0021968162901169), *J. Chronic Dis.*</small>
:::

---

## 1 ΔNCPV as Outcome Variable {.smaller}

```r
n <- 100 # Set the number of values to generate
baseline <- rnorm(n, mean = 100, sd = 10) # Create 100 random numbers centered around 100
follow_up <- rnorm(n, mean = 120, sd = 10) # Create another 100 random numbers, centered around 110  
delta <- follow_up - baseline # Subtract the first set from the second to get the difference

```
<br>

::: {.fragment}

::: columns

::: {.column width="50%"}

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.cap="No true association"}

library(ggplot2)
library(plotly)

set.seed(42)

n <- 100
baseline <- rnorm(n, mean = 100, sd = 10)
follow_up <- rnorm(n, mean = 120, sd = 10)
delta <- follow_up - baseline

df <- data.frame(
  baseline = baseline,
  follow_up = follow_up,
  delta = delta
)

model_raw <- lm(follow_up ~ baseline, data = df)
slope_raw <- coef(model_raw)[2]
pval_raw <- summary(model_raw)$coefficients[2, 4]
pval_raw_text <- format.pval(pval_raw, eps = 0.001)
label_raw <- sprintf("β = %.2f, p %s", slope_raw, pval_raw_text)

p_raw = ggplot(df, aes(x = baseline, y = follow_up)) +
  geom_point(alpha = 0.7, size = 2.5) +
  geom_smooth(method = "lm", se = FALSE, color = "steelblue") +
  annotate("text", x = min(df$baseline) + 30, y = max(df$follow_up),
           label = label_raw, color = "darkred", hjust = 0, size = 4) +
  labs(
    title = "Basline vs Follow-up",
    subtitle = "Two independent variables — no true effect",
    x = "Baseline",
    y = "Follow up"
  ) +
  theme_minimal()

ggplotly(p_raw, height = 350, width = 500) %>%
  config(displayModeBar = FALSE) 

```
:::

::: {.column width="50%" .fragment}

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Association due to mathematical coupling"}


model_change <- lm(delta ~ baseline, data = df)
slope_change <- coef(model_change)[2]
pval_change <- summary(model_change)$coefficients[2, 4]
pval_change_text <- format.pval(pval_change, eps = 0.001)
label_change <- sprintf("β = %.2f, p %s", slope_change, pval_change_text)


p = ggplot(df, aes(x = baseline, y = delta)) +
  geom_point(alpha = 0.7, size = 2.5) +
  geom_smooth(method = "lm", se = FALSE, color = "steelblue") +
  annotate("text", x = min(df$baseline) + 30, y = max(df$delta),
           label = label_change, color = "darkred", hjust = 0, size = 4) +
  labs(
    title = "Basline vs Change (Follow up - Baseline)",
    subtitle = "Mathematical Coupling Creates a Spurious Slope",
    x = "Baseline",
    y = "Follow up - Baseline (Δ)"
  ) +
  theme_minimal()

ggplotly(p, height = 350, width = 500) %>%
  config(displayModeBar = FALSE)

```

:::

:::

:::

---

## 1 ΔNCPV as Outcome Variable {.smaller}


An alternative is to model NCPV at follow-up ($\text{NCPV}_1$) directly while adjusting for baseline NCPV. This example uses ApoB as the independent variable:

$$
\text{NCPV}_1 = \alpha + \gamma\,\text{NCPV}_0 + \beta ApoB + \varepsilon
$$

This approach avoids mathematical coupling, reduces residual variance, and allows the coefficient on $ApoB$ to reflect biological association — not algebraic structure.


<br>

::: {.fragment}


You could test whether baseline NCPV predicts follow-up using a mixed-effects model with a Time × Baseline interaction:

> $$
> \text{NCPV}_{ij} = \alpha + \gamma\,\text{Time}_{ij} + \beta\,\text{NCPV}_{0j} + \delta\,(\text{Time}_{ij} \cdot \text{NCPV}_{0j}) + b_j + \varepsilon_{ij}
> $$

but the with the number of subjects this must be done carefully.

:::

---

## 1 ΔNCPV as Outcome Variable {.smaller}

<div style="font-size: 90%; line-height: 0.8;">
You cannot determine whether a positive or negative slope from ΔNCPV ∼ NCPV₀ reflects biology or math, because the math builds the relationship.
To isolate biological effects, you must model follow-up directly with baseline as a covariate — not as part of the outcome.

<br>

::: {.fragment}

["Statisticians have repeatedly warned against correlating/regressing change with baseline due to two methodological concerns known as mathematical coupling and regression to the mean."](https://pubmed.ncbi.nlm.nih.gov/16526009/) - Tu & Gilthorpe, 2007

["Mathematical coupling can lead to an artificially inflated association between initial value and change score when correlation or regression is used."](https://www.sciencedirect.com/science/article/abs/pii/S0895435609003497#:~:text=Mathematical%20coupling%20occurs%20%E2%80%9Cwhen%20one%20variable%20directly,posttreatment%20score%2C%20they%20contain%20the%20pretreatment%20score.) - Brown, 2009

:::

</div>



::: {.fragment}

<br>
<br>
**Plaque begets plaque**: biology or a mathematical artifact?

:::

<br>

::: {.fragment}

<div style="font-size: 50%; margin-top: 1em;">

**See also:**  
[Analysis of 'change scores'](https://clok.uclan.ac.uk/38260/1/38260%20dyab050.pdf)  
[Assessing the Relationship between the Baseline Value of a Continuous Variable and Subsequent Change Over Time](https://pmc.ncbi.nlm.nih.gov/articles/PMC3854983/)  
[Mathematic coupling of data: a common source of error](https://pmc.ncbi.nlm.nih.gov/articles/PMC1345065/)
[Revisiting the relation between change and initial value: a review and evaluation](https://onlinelibrary.wiley.com/doi/10.1002/sim.2538)

</div>

:::

# Univariable Linear Models
At best: exploratory. 

At worst: misleading.

## 2 Univariable Linear Models {.smaller}

> "Linear models on the primary (NCPV) and secondary outcomes were **univariable**"

::: {.fragment}

Despite having multiple predictors available (age, sex, ApoB, BMI, Triglycerides, Systolic blood pressure, CAC, NCPV₀, LDL-C exposure),  
each was tested separately in single-predictor regressions.

:::

::: {.fragment}

This modeling choice introduces **omitted-variable bias**:  

> "...omitting a relevant variable from a model which explains the independent and dependent variable leads to biased estimates."  
> [Wilms (2021)](https://www.researchgate.net/publication/354514325_Omitted_variable_bias_A_threat_to_estimating_causal_relationships)

When a predictor is correlated with both the outcome and another omitted variable, its coefficient may absorb the effect of that omitted factor.

:::

---

## 2 Univariable Linear Models {.smaller}

#### Example: ΔNCPV, ApoB and Age 

They modeled:

$$
\Delta \text{NCPV} = \alpha + \beta \text{ApoB} + \varepsilon
$$

::: {.fragment}

But if **age** also predicts ΔNCPV and correlates with ApoB, then $\beta$ is **biased** — it partly reflects the effect of age.


A more appropriate model would be:

$$
\Delta \text{NCPV} = \alpha + \beta_1 \text{ApoB} + \beta_2 \text{Age} + \varepsilon
$$

:::

::: {.fragment}

This separates the contribution of ApoB from that of age.

:::
::: {.fragment}

<br>
Univariable Linear Models are make confounding almost certain, especially in small, non-randomized human data, undermining any claim of association or non-association.

:::

## 2 Univariable Linear Models {.smaller}

But wait:
<div style="font-size: 75%; margin-top: 1em;">
> "Estimated lifetime LDL-C exposure was only a significant predictor of final NCPV in the univariable analysis but lost significance when age was included as a covariate. Both age and lifetime LDL-C exposure lost significance when baseline CAC was included in the model."
</div>

::: {.fragment}

So they **did use multivariable models**! but only on the follow-up NCPV, not for ΔNCPV, the paper's main endpoint?

<br>
This selective use raises questions:

<ul style="margin-top: 0; margin-bottom: 0; line-height: 0.85;">
  <li>Why not model ΔNCPV with adjustment for known confounders?</li>
  <li>Why apply different modeling rules depending on the predictor?</li>
  <li>Why include some adjusted models in the paper?</li>
</ul>

<br>

:::

::: {.fragment}

Were multivariable models used selectively for some reason? And why not on ΔNCPV?

:::

## 2 Univariable Linear Models {.smaller}

> "Neither change in ApoB...baseline ApoB, nor total LDL-C exposure... were associated with the change in noncalcified plaque volume (NCPV) or TPS. All baseline plaque metrics (coronary artery calcium, NCPV, total plaque score, and percent atheroma volume) were strongly associated with the change in NCPV."

::: {.fragment}

<br>
Stating this in the **abstract**, based solely on univariable regressions, is a clear case of overstated, methodologically flawed inference.
:::
::: {.fragment}

<br>
At best, this reflects naïve reporting; at worst, it's actively misleading.
:::

# Bayesian Modeling 
Unusual. Fragile. Overstated.


## 3 Bayesian Inference {.smaller}

<div style="font-size: 75%;">

**Frequentist:**

::: {.fragment}

Assuming there is no true association between ApoB and ΔNCPV, how likely is it that we'd observe a slope as large (or larger) than the one we found, just by chance?

If p-value (p > 0.05) a frequentist analysis can say:

*"We did not find sufficient evidence to reject the hypothesis that ApoB has no association with ΔNCPV"*

It **cannot** say the null is likely true, or produce the probability that there is no association, just that the data were inconclusive.

:::

<br>
**Bayesian:**

::: {.fragment}

How well do the data fit under two competing models, one with no association (null), and one with a range of plausible effect sizes for ApoB (alternative)?

A **Bayes factor (e.g., BF₁₀ = 6)** allows a stronger statement:

*"The observed data are 6 times more likely (moderate evidence) under the 'no association' model than under the alternative model that assumes some effect from ApoB (as defined by the prior)."*

:::

::: {.fragment}

- **Frequentist**: *"No evidence of effect"*  
- **Bayesian**: *"Evidence for no effect"*
:::
</div>

---

## 3 Bayesian Inference {.smaller}

<div style="font-size: 80%;">

> "Since lack of statistical signifcance (ie, P > 0.05) should not be interpreted as evidence in favor of the null but simply a failure to reject the null, the addition of Bayesian inference adds credence to finding that there is no association between NCPV vs LDL-C or ApoB..."

</div>

::: {.fragment}

So, they turn to Bayesian inference to "support" their finding that **ApoB has no association with plaque progression**.

:::

::: {.fragment}

<br>
This is *unusual* in a non-randomized, uncontrolled, 1-year observational study on a highly restricted sample:

<ul style="margin-top: 0; margin-bottom: 0; line-height: 0.9; font-size: 80%; padding-left: 1.5em;">
  <li>Study design not suited for strong inferences about presence or absence of associations</li>
  <li>Univariable, unadjusted models: reduce credibility of any statistical conclusion</li>
  <li>Bayesian inference used to imply [absence of effect, not just lack of evidence](https://orca.cardiff.ac.uk/id/eprint/163206/4/tendeiro-et-al-2024-diagnosing-the-misuse-of-the-bayes-factor-in-applied-research.pdf#page=6&search=Presence%20versus%20absence)</li>
</ul>

:::

::: {.fragment}

<br>
Despite the limited model and context, they present the result as confirmatory.

:::

---


## 3 Bayesian Inference {.smaller}

<br>
They are applying a stronger-sounding statistical framework onto a structurally weak analysis.

::: {.fragment}

This is a **misuse** of Bayesian inference. 

:::

::: {.fragment}

<br>

Not because Bayesian methods are invalid.

<br>
Because they're being used to amplify certainty in an analysis that lacks adjustment, control, or transparency about its assumptions.

:::
---

## 4 Bayesian Prior Choice {.smaller}


<div style="font-size: 73%;">

> "Bayes factors were calculated using BayesFactor::regressionBF... and an ~ rscale value of 0.8 to *contrast a moderately informative prior with a conservative distribution width* (to allow for potential large effect sizes) due to the well-documented association between ApoB changes and coronary plaque changes"

</div>


<div style="font-size: 90%;">

::: {.fragment}

<br>
**Bayesian Prior**: represents your belief about likely effect sizes before seeing the data.

From the [BayesFactor documentation](https://cran.r-project.org/web/packages/BayesFactor/BayesFactor.pdf) for the parameter `rscaleCont`:

"Several named values are recognized: [‘medium’, ‘wide’, and ‘ultrawide’](https://richarddmorey.github.io/BayesFactor/#proptest), which correspond to rscales of √2/4, 1/2, and √2/2, respectively.”

<ul style="margin-top: 0; margin-bottom: 0; line-height: 0.9; font-size: 80%; padding-left: 1.5em;">
  <li>**"medium"** → rscale = √2 / 4 ≈ **0.354**</li>
  <li>**"wide"** → rscale = **0.5**</li>
  <li>**"ultrawide"** → rscale = √2 / 2 ≈ **0.707**</li>
</ul>

:::

::: {.fragment}

`rscale` of **0.8** is **wider than "ultrawide"**. It is not a "moderately informative" prior. It's actually a weakly informative or vague prior, placing most of its weight on large effects.

A moderately informative prior would typically correspond to "medium" (≈ 0.354) or "wide" (0.5), which place more mass on smaller effects.

:::

</div>

---

## 4 Bayesian Prior Choice {.smaller}

The authors' prior choice isn't wrong, but their description of it is misleading.

Labeling an **r = 0.8** prior as “moderately informative” or “conservative” downplays the fact that it assumes **large effects**, making small observed effects look unlikely under H₁ and inflating support for H₀.

<br>

::: {.fragment}

Their choice of prior is subjective, influential, and not tested for robustness.

- The model was set up to expect large ApoB effects, so small observed effects are treated as evidence for no effect. 

- **Best practice** is to run a **sensitivity analysis**, to see whether conclusions change with different priors.

:::

::: {.fragment}


<div style="font-size: 80%;">

["it is always important to conduct a prior **sensitivity analysis** to fully understand the influence that the prior settings have on posterior estimates"](https://www.nature.com/articles/s43586-020-00001-2)

["a researcher can have a very strong opinion about the model parameter values, and this opinion (via the prior) can drive the final model estimates."](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2020.608045/full)

</div>
:::
---

## 4 Bayesian Prior Choice {.smaller}

#### Prior Scale Sensitivity Analysis on ΔNPCV ~ ApoB Model

<br>

::: columns

::: {.column width="40%"}

```{r echo=FALSE, message=FALSE, warning=FALSE}

library(BayesFactor)
library(kableExtra)

# Simulated data
set.seed(123)
n <- 100  # Match study sample size

# ApoB in mg/dL, realistic scale from study
apoB <- rnorm(n, mean = 185, sd = 50.8)

# Simulate ΔNCPV with very weak ApoB effect (like β = 0.1), and large residual SD
delta_NCPV <- pmax(0, 0.1 * apoB + rnorm(n, mean = 40, sd = 70))

# Combine into dataframe
df <- data.frame(delta_NCPV, apoB)

# View correlation and regression summary
# summary(lm(delta_NCPV ~ apoB, data = df))

# rscale values to test
rscales <- c(0.1, 0.25, 0.35, 0.5, 0.707, 0.8, 1.0)
results <- sapply(rscales, function(r) {
  bf <- regressionBF(delta_NCPV ~ apoB, data = df, rscaleCont = r)
  extractBF(bf, onlybf = TRUE)
})

sensitivity_df <- data.frame(
  rscale = rscales,
  BF10 = as.numeric(results),
  BF01 = 1 / results
)

kable(sensitivity_df, digits = 3,
      col.names = c("rscale", "BF₁₀", "BF₀₁"),
      caption = "Bayes factor sensitivity analysis") %>%
  kable_styling(full_width = FALSE, font_size = 22)

```

:::

::: {.column width="60%"}

```{r echo=FALSE, message=FALSE, warning=FALSE}

# Plot BF01 vs. rscale
prior_sensitivity = ggplot(sensitivity_df, aes(x = rscale, y = BF01)) +
  geom_line(linewidth = 1.2, color = "steelblue") +
  geom_point(size = 3, color = "steelblue") +
  geom_hline(yintercept = 3, linetype = "dashed", color = "gray40") +
  geom_hline(yintercept = 10, linetype = "dotted", color = "gray40") +
  annotate("text", x = 0.4, y = 3.3, label = "Moderate evidence", hjust = 0, size = 3.5) +
  annotate("text", x = 0.4, y = 10.3, label = "Strong evidence", hjust = 0, size = 3.5) +
  labs(
    title = "Bayes Factor Sensitivity to Prior Width (rscale)",
    x = "Prior rscale",
    y = "Bayes Factor (BF₀₁)",
  ) +
  theme_minimal(base_size = 13)

ggplotly(prior_sensitivity , height = 375, width = 550) %>%
  config(displayModeBar = FALSE) 

```

:::

:::

<div style="font-size: 75%;">
This kind of [rscale sensitivity analysis](https://link.springer.com/article/10.3758/PBR.16.2.225) is standard for default Bayes factors, but it's a limited diagnostic — it tests only prior width, not prior plausibility or model fit.
</div>

---

## 5 Bayes Factor Interpretation {.smaller}

>"In other words, these data suggest it is 6 to 10 times more likely that the hypothesis of no association between these variables (the null) is true as
compared to the alternative."

::: {.fragment}

[That is a overstatement of what the Bayes Factor tells us.](https://lakens.github.io/statistical_inferences/04-bayes.html#:~:text=One%20might%20therefore%20believe%20that%20Bayes%20factors%20tell%20us%20something%20about%20the%20probability%20that%20a%20hypothesis%20true%2C%20but%20this%20is%20incorrect.)

:::

::: {.fragment}

A Bayes factor of 6–10 means the **data** are 6–10× more likely under the **null model** than under the **alternative model**, not that the *null hypothesis is 6–10×* more likely to be true.


They could have said: "The data are 6–10 times more likely under the no-association model than under the alternative"

:::

::: {.fragment}

$$
\text{Posterior Odds} = \text{Bayes Factor} \times \text{Prior Odds}
$$

To claim that the null is 6–10× more likely to be *true*, they would have to assume **prior odds = 1:1** — and state that explicitly. They didn’t.

<small>Source: [Bayes Factors – Kass Raferty, 1995](https://www.nbi.dk/~koskinen/Teaching/AdvancedMethodsInAppliedStatistics2018/StudentPresentations/Bayes_factors_Jonas_S_Juul.pdf)</small>

:::

## 5 Bayes Factor Interpretation {.smaller}

Even without other issues (e.g. confounding / non-adjusted variables / short follow-up / non-RCT, etc.),
the reported BF of **6.3 for ΔNCPV ~ ApoB** reflects only moderate evidence for no effect —
not strong or decisive.


<img src="assets/BF_interpretation.png" width="40%" style="display: block; margin: auto;" />

<div style="font-size: 60%; text-align: center; margin-top: 0.5em;">
<strong>Table 1.</strong> A heuristic classification scheme for Bayes factors <strong>BF<sub>10</sub></strong> 
Source: <a href="https://link.springer.com/content/pdf/10.3758/s13428-018-01189-8.pdf">SpringerLink</a>
</div>



<small>
(Assuming BF₁₀ as per standard conventions; if BF₀₁, it reverses)
</small>

---

## Summary {.smaller}

<div style="font-size: 82%; line-height: 1.2;">

::: {.fragment}

1. **ΔNCPV as outcome**  
   Regressed (NCPV₁ − NCPV₀) on baseline NCPV₀ <br>
   Mathematical coupling → regression slope reflect **algebra, not just biology**
:::
::: {.fragment}

2. **Univariable regressions**  
   Each predictor tested separately (ApoB, LDL-C, age…)  
   No confounder adjustment → **biased, unreliable, low credibility estimates**
:::
::: {.fragment}

3. **Bayesian inference**  
   Bayes inference used to support "no ApoB effect" & "plaque begets plaque"<br> 
   Unadjusted, observational data → **misleading, unusual use of Bayesian inference**
:::
::: {.fragment}

4. **Prior choice (rscale = 0.8)**  
   Prior assumes large effects<br>
   No sensitivity analysis → results likely **prior-driven**
:::
::: {.fragment}

5. **Bayes factor interpretation**  
   Claimed null is "6–10× more likely"<br>
   Bayes factor misstated as posterior probability → compares model fit, **not truth**
:::
::: {.fragment}

6. **Headline claim**  
   "Plaque Begets Plaque, ApoB Does Not"<br>
   Overstates evidence → **mathematically coupled, confounded, and fragile analysis**
:::

</div>


# Additional Concerns

---

## No Adjustment for Multiple Comparisons {.smaller}

At least 14 distinct linear regressions were reported, with likely many more from exploratory models referenced in figures and supplements.

Numerous tests increase false positive risk, yet no multiple testing correction was applied.

::: {.fragment}

Such as:


* Bonferroni: α = 0.05 → α′ ≈ 0.005  
  Baseline CAC (P < 0.001) would survive; others might not  
* Benjamini–Hochberg FDR maintains power with correlated tests

:::

::: {.fragment}

Perhaps the authors viewed this as exploratory, where correction is often skipped —  
**but then why title the paper "Plaque Begets Plaque, ApoB Does Not"?**

:::

---

## Zero-inflation, censoring, heteroscedasticity {.smaller}

<div style="font-size: 80%;">
Baseline NCPV median = 44 mm³; **TPS median = 0** → ≥50% of values are zero  
CCTA cannot report negative plaque → both outcomes are **left-censored at 0**

This affects not just modeling but measurement:  
When true plaque ≈ 0, **error is asymmetric** — it can only overestimate.

::: {.fragment}

ΔNCPV, their primary outcome, is a change score between two bounded, skewed measures.  
Likely to produce **non-normal residuals** and **heteroscedasticity** (e.g., larger spread at higher baseline).

> If smaller baseline values were also linked to larger increases,  
> this may reflect the effects of left-censoring and error asymmetry — not true biological acceleration.

:::

::: {.fragment}

These issues are **clear in TPS**, and may affect NCPV, but diagnostics are not shown.

*OLS assumes homoscedastic, normal residuals*  
*performance::check_model() was run — but no output provided*

They could have considered methods to address this such as: **Tobit regression**, **log-transform**, or **robust SEs**

:::

</div>


## Possibly Underpowered Study {.smaller}

<div style="font-size: 75%; margin-bottom: 1em">
<ul style="margin-top: 0; margin-bottom: 0; line-height: 1;">
  <li>Study Registered Primary endpoint: **%Δ NCPV over 12 mo**, not specifically sized for ApoB detection.</li>
  <li>With *n = 100*, 80 % power is reached for large and medium effects for 1 predictor linear regression.</li>
  <li>A null result after one year may be due to low power, not proof of no ApoB effect.</li>    
  <li>Note: even if it is adequately powered, it doesn't negate all the other issues like mathematical coupling, confounding, non-adjustment, short follow-up etc.</li>
</ul>
</div>

```{r echo=FALSE, message=FALSE, warning=FALSE}

library(ggplot2)
library(pwr)
library(plotly)

# Define effect sizes and parameters
effect_sizes <- c(small = 0.02, medium = 0.15, large = 0.35)
u <- 1
alpha <- 0.05
n_vals <- seq(10, 600, by = 1)

# Calculate power for each effect size
power_df <- data.frame(
  SampleSize = rep(n_vals, times = length(effect_sizes)),
  EffectSize = rep(names(effect_sizes), each = length(n_vals)),
  Power = NA_real_
)

for (i in seq_len(nrow(power_df))) {
  f2 <- effect_sizes[power_df$EffectSize[i]]
  n <- power_df$SampleSize[i]
  v <- n - u - 1
  power_df$Power[i] <- pwr.f2.test(u = u, f2 = f2, sig.level = alpha, v = v)$power
}

# Base plot
power_curves <- ggplot(power_df, aes(x = SampleSize, y = Power, color = EffectSize)) +
  geom_line(size = 1.2) +
  geom_hline(yintercept = 0.8, linetype = "dashed", color = "black") +
  geom_vline(xintercept = 100, linetype = "dashed", color = "gray30") +
  labs(
    title = "Power Curve for Linear Regression (1 Predictor)",
    subtitle = "Effect Sizes: Small (f² = 0.02), Medium (f² = 0.15), Large (f² = 0.35)",
    x = "Sample Size", y = "Power", color = "Effect Size"
  ) +
  theme_minimal(base_size = 14) + 
  theme(legend.position = "bottom")

# Convert to interactive plot
power_plotly <- ggplotly(power_curves, height = 450, width = 800) %>%
  layout(
    annotations = list(
      list(
        x = 550, y = 0.82, text = "80% Power Threshold", 
        showarrow = TRUE, arrowhead = 2, ax = -40, ay = -30
      ),
      list(
        x = 100, y = 0.25, text = "Study Sample Size = 100", 
        showarrow = TRUE, arrowhead = 2, ax = 40, ay = -40
      )
    )
  )

power_plotly 

```



## On the Heterogeneity of LMHR {.smaller}

<div style="font-size: 85%">

> "It should be emphasized that this includes heterogeneity in progression (and regression) across the population." - Keta-CTA study

> ["If, despite our results show that CVDrisk among LMHRs is heterogeneous (and thus, **a pooled summary isn't a good idea**), you must have a numerical pooled NCPVchange value, it is: p50=18.8 mm3 IQR(37.3)."](https://x.com/AdrianSotoMota/status/1910045858152042999) - X post from Author

::: {.fragment}

<br>
They say the group is heterogeneous (to downplay the pooled NCPV change), yet they ran univariable regressions and interpreted pooled Bayes factors as if the group were homogeneous.

:::
::: {.fragment}

<br> 
If a group is too heterogeneous to report pooled outcomes, it is also too heterogeneous to justify pooled inferences about predictors or mechanisms.

If their CVD risk (e.g., plaque progression) is not coherent, then the category fails as a predictive or explanatory tool.

:::

</div>

---

## On the Heterogeneity of LMHR {.smaller}

<div style="font-size: 85%">


> "p50=18.8 mm3 IQR(37.3)."

With a median of 18.8 mm³ and only 1–2 individuals showing regression, the IQR of 37.3 mm³ must be mostly skewed upward, not balanced.

A wide IQR here reflects high inter-individual variability in outcomes among the LMHRs, variability in progression of NCPV, as all but a few individuals had more plaque at follow up.

::: {.fragment}

In other words: With nearly all LMHRs showing plaque progression, a wide IQR (37.3 mm³) doesn’t indicate balanced variability—it reflects differing degrees of worsening.

:::
::: {.fragment}

<br>

This matters because:

<ul style="margin-top: 0; margin-bottom: 0; line-height: 1; padding-left: 1.5em;">
  <li>The outcome (ΔNCPV) shows high variability across individuals in the LMHR group.</li>
  <li>That inflates standard errors, weakens statistical power, and makes small observed effects more ambiguous.</li>
  <li>It calls for adjusted modeling to reduce noise and account for confounding, not pooled univariable regressions and Bayes factors interpreted as strong evidence.</li>
</ul>

:::

</div>

---

## Notes on Letters to the Editor {.smaller}

<div style="font-size: 75%">

[Letter to the Editor](https://www.jacc.org/doi/epdf/10.1016/j.jacadv.2025.101861) - External researchers raise concerns about the study’s methodology

[Response to the Letter](https://www.jacc.org/doi/epdf/10.1016/j.jacadv.2025.101862) - Study authors respond to concerns

::: {.fragment}

From the response:

>"Regarding the analytical points brought forward, we are aware of the relevance of linear assumptions to obtain accurate estimators. Since **residual plot evaluation** can also be **subjective**..."

:::

::: {.fragment}

Objective, quantitative statistical tests also exist for assessing model assumptions:

<ul style="margin-top: 0; margin-bottom: 0; font-size: 85%; line-height: 1; padding-left: 1.5em;">
  <li>Normality: Shapiro-Wilk, Kolmogorov–Smirnov, Q-Q plots (visual / quantitative)</li>
  <li>Homoscedasticity: Breusch–Pagan, White test</li>
  <li>Influence / leverage: Cook’s distance, leverage plots (visual based on quantitative values)</li>
</ul>

The `performance::check_model()` function they cite automatically generates most of these checks.

:::
::: {.fragment}

It’s unclear why they wouldn't include or reference the output.

:::

::: {.fragment}

> "...we followed their suggestion and re-ran all models with robust linear regression...as expected, there were small differences with the published estimates, all models using robust regression were consistent with what was reported."

:::
::: {.fragment}

No output, diagnostics, or model fit provided. We are asked to trust their assertion.

:::

</div>

---

## Notes on Letters to the Editor {.smaller}

<div style="font-size: 90%">

>"We agree that being able to identify patients with rapid plaque progression, and gaining better understanding of the mechanisms that mediate its pace (i.e. insulin resistance, inflammation, different dietary composition elements, etc.) is paramount. We plan to address these risk factors in future reports"

::: {.fragment}

Are they admitting their analysis lacks adjustment for likely confounders, despite drawing strong conclusions from univariable regressions?

Just run and report the multivariable, adjusted models. 

:::

::: {.fragment}

<br>

>"Moreover, our results are compatible with a causal role of ApoB in atherosclerosis, as we have openly acknowledged and supported in previous publications."

:::
::: {.fragment}

<br>

**"Plaque Begets Plaque, ApoB does not"**

:::

</div>

---

## Notes on Letters to the Editor {.smaller}

<div style="font-size: 85%">


>"Along the same lines, we would like to clarify that our title was not meant to be a statement about causality. “Plaque begets plaque” (which, of course, mirrors the proverb “Money begets money”) is frequently used to highlight the strong and clinically relevant association of baseline plaque values with plaque progression rate [[7]](https://pubs.rsna.org/doi/abs/10.1148/radiol.2241011191?journalCode=radiology). 

::: {.fragment}

That journal citation used a random-effects repeated-measures model, a type of longitudinal multivariable regression that accounts for the non-independence of repeated observations within individuals, and controlled for baseline calcium score and traditional risk factors. 

This approach was chosen after detecting heteroskedasticity in a preliminary multivariable linear regression. Both models were multivariable (adjusted), not univariable.

:::

<br>

::: {.fragment}

>"In retrospect, we might have chosen “Longitudinal Data from the KETO-CTA Study” as alternative phrasing to avoid **misinterpretations.**"

:::

</div>


---


## Notes on Letters to the Editor {.smaller}

<div style="font-size: 130%">

> "misinterpretations"

</div>

::: {.fragment}

<a href="https://dictionary.cambridge.org/us/dictionary/english/beget" target="_blank">
  <img src="assets/beget.png" width="70%" style="display: block; margin: auto;" />
</a>

:::

